{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0pqnoQvn-c6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "from random import random\n",
        "# Importing all the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWVxog_Qn-c_"
      },
      "outputs": [],
      "source": [
        "def patchify(images, n_patches):\n",
        "    n, c, h, w = images.shape\n",
        "\n",
        "    assert h == w, \"Patchify method is implemented for square images only\"\n",
        "\n",
        "    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2) # Initialisation of the tensor with zeroes\n",
        "    patch_size = h // n_patches\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        for i in range(n_patches):\n",
        "            for j in range(n_patches):\n",
        "                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
        "                patches[idx, i * n_patches + j] = patch.flatten()\n",
        "    return patches\n",
        "# The function divides the images into patches and then flattens the formed 3D vectors into flattened 1D vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQP8WSC-n-dA"
      },
      "outputs": [],
      "source": [
        "class MyMSA(nn.Module):\n",
        "    def __init__(self, d, n_heads=8):\n",
        "        super(MyMSA, self).__init__()\n",
        "        self.d = d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
        "\n",
        "        d_head = int(d / n_heads)\n",
        "        self.q_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.k_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.v_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.d_head = d_head\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        # Sequences has shape (N, seq_length, token_dim)\n",
        "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
        "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
        "        result = []\n",
        "        for sequence in sequences:\n",
        "            seq_result = []\n",
        "            for head in range(self.n_heads):\n",
        "                q_mapping = self.q_mappings[head]\n",
        "                k_mapping = self.k_mappings[head]\n",
        "                v_mapping = self.v_mappings[head]\n",
        "\n",
        "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
        "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
        "\n",
        "                attention = self.softmax(q @ k.T / (self.d_head**0.5))\n",
        "                seq_result.append(attention @ v)\n",
        "            result.append(torch.hstack(seq_result))\n",
        "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTer8YUCn-dB"
      },
      "outputs": [],
      "source": [
        "class MyViTBlock(nn.Module):\n",
        "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
        "        super(MyViTBlock, self).__init__()\n",
        "        self.hidden_d = hidden_d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_d)\n",
        "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_d)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.mhsa(self.norm1(x))\n",
        "        out = out + self.mlp(self.norm2(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znkAyT-Qn-dC"
      },
      "outputs": [],
      "source": [
        "def get_positional_embeddings(sequence_length, d):\n",
        "    result = torch.ones(sequence_length, d)\n",
        "    for i in range(sequence_length):\n",
        "        for j in range(d):\n",
        "            result[i][j] = (\n",
        "                np.sin(i / (10000 ** (j / d)))\n",
        "                if j % 2 == 0\n",
        "                else np.cos(i / (10000 ** ((j - 1) / d)))\n",
        "            )\n",
        "    return result\n",
        "# This function just calculates the positional embeddings and uses the trigonometric functions for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ5Y_3Nvn-dC"
      },
      "outputs": [],
      "source": [
        "# Helper function to plot attention maps\n",
        "def plot_attention_maps(attention_maps, title):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i, attention_map in enumerate(attention_maps):\n",
        "        plt.subplot(1, len(attention_maps), i + 1)\n",
        "        plt.imshow(attention_map.squeeze().cpu().numpy(), cmap='viridis', interpolation='nearest')\n",
        "        plt.title(f'Attention Map {i + 1}')\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QNBonZFn-dD"
      },
      "outputs": [],
      "source": [
        "class MyViT(nn.Module):\n",
        "    def __init__(self, chw, n_patches=8, n_blocks=4, hidden_d=8, n_heads=8, out_d=10):\n",
        "        # Super constructor\n",
        "        super(MyViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.chw = chw  # ( C , H , W )\n",
        "        self.n_patches = n_patches\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_d = hidden_d\n",
        "\n",
        "        # Input and patches sizes\n",
        "        assert (chw[1] % n_patches == 0 ), \"Input shape not entirely divisible by number of patches\"\n",
        "        assert (chw[2] % n_patches == 0 ), \"Input shape not entirely divisible by number of patches\"\n",
        "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
        "\n",
        "        # 1) Linear mapper\n",
        "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
        "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
        "\n",
        "        # 2) Learnable classification token\n",
        "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
        "\n",
        "        # 3) Positional embedding\n",
        "        self.register_buffer(\n",
        "            \"positional_embeddings\",\n",
        "            get_positional_embeddings(n_patches**2 + 1, hidden_d),\n",
        "            persistent=False,\n",
        "        )\n",
        "\n",
        "        # 4) Transformer encoder blocks\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)]\n",
        "        )\n",
        "\n",
        "        # 5) Classification MLPk\n",
        "        self.mlp = nn.Sequential(nn.Linear(self.hidden_d, out_d), nn.Softmax(dim=-1))\n",
        "        self.attention_maps = []\n",
        "    def forward(self, images):\n",
        "        # Dividing images into patches\n",
        "        n, c, h, w = images.shape\n",
        "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
        "\n",
        "        # Running linear layer tokenization\n",
        "        # Map the vector corresponding to each patch to the hidden size dimension\n",
        "        tokens = self.linear_mapper(patches)\n",
        "\n",
        "        # Adding classification token to the tokens\n",
        "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
        "\n",
        "        # Adding positional embedding\n",
        "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
        "\n",
        "\n",
        "        # Transformer Blocks\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "\n",
        "\n",
        "        # Getting the classification token only\n",
        "        out = out[:, 0]\n",
        "\n",
        "        return self.mlp(out)  # Map to output dimension, output category distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKy0cFdGn-dE"
      },
      "outputs": [],
      "source": [
        "def get_attention_maps(model, images, n_patches=8):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Convert images to tensor and move to the appropriate device\n",
        "    images = torch.tensor(images).unsqueeze(0).to(model.positional_embeddings.device)\n",
        "\n",
        "    # Dividing images into patches\n",
        "    patches = patchify(images, n_patches).to(model.positional_embeddings.device)\n",
        "\n",
        "    # Running linear layer tokenization\n",
        "    tokens = model.linear_mapper(patches)\n",
        "\n",
        "    # Adding classification token to the tokens\n",
        "    tokens = torch.cat((model.class_token.expand(1, 1, -1), tokens), dim=1)\n",
        "\n",
        "    # Adding positional embedding\n",
        "    out = tokens + model.positional_embeddings.repeat(1, 1, 1)\n",
        "\n",
        "    # Transformer Blocks\n",
        "    attention_maps = []\n",
        "    for block in model.blocks:\n",
        "        out = block.mhsa(out)  # Get attention maps from the MyMSA module\n",
        "        attention_maps.append(out[0, :, 1:, :].detach().cpu().numpy())  # Extract attention maps, excluding the classification token\n",
        "\n",
        "    return attention_maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEJDoLy5n-dE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def main():\n",
        "    # Loading data\n",
        "    transform = ToTensor()\n",
        "\n",
        "    train_set = CIFAR10(\n",
        "        root=\"./../datasets\", train=True, download=True, transform=transform\n",
        "    )\n",
        "    test_set = CIFAR10(\n",
        "        root=\"./../datasets\", train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
        "\n",
        "    # Defining model and training options\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\n",
        "        \"Using device: \",\n",
        "        device,\n",
        "        f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
        "    )\n",
        "    model = MyViT(\n",
        "        (3, 32, 32), n_patches=8, n_blocks=4, hidden_d=8, n_heads=8, out_d=10\n",
        "    ).to(device)\n",
        "    N_EPOCHS = 2\n",
        "    LR = 0.01\n",
        "\n",
        "    # Training loop\n",
        "    optimizer = Adam(model.parameters(), lr=LR)\n",
        "    criterion = CrossEntropyLoss()\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        train_loss = 0.0\n",
        "        num_batches = len(train_loader)\n",
        "        print()\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "\n",
        "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            completion_percentage = (batch_idx + 1) / num_batches * 100\n",
        "            print(f\"\\rEpoch {epoch + 1}/{N_EPOCHS} [{completion_percentage:.2f}%] - Loss: {train_loss:.2f}\", end='')\n",
        "\n",
        "    # Test loop\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        test_loss = 0.0\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "\n",
        "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
        "            total += len(x)\n",
        "        print(f\"Test loss: {test_loss:.2f}\")\n",
        "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
        "\n",
        "    # Get attention maps for sample images\n",
        "    with torch.no_grad():\n",
        "        sample_images = torch.stack([test_set[i][0] for i in range(4)])\n",
        "        attention_maps = get_attention_maps(model, sample_images)\n",
        "\n",
        "        # Plot attention maps\n",
        "        for i, attention_map in enumerate(attention_maps):\n",
        "            plt.figure(figsize=(15, 5))\n",
        "            for j in range(len(attention_map)):\n",
        "                plt.subplot(1, len(attention_map), j + 1)\n",
        "                plt.imshow(attention_map[j], cmap='viridis', interpolation='nearest')\n",
        "                plt.title(f'Block {j + 1}')\n",
        "                plt.colorbar()\n",
        "            plt.suptitle(f'Attention Maps - Sample {i + 1}')\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqGEYczLn-dF"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD7uqPhJn-dH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}